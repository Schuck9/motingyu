<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  
  <style>
    .tooltip {
      position: relative;
    }

    .tooltip .tooltiptext {
      visibility: hidden;
      width: 100px;
      background-color: white;
      color: #000;
      text-align: center;
      border-radius: 6px;
      padding: 5px 0;

      /* Position the tooltip */
      position: absolute;
      z-index: 1;
      top: -5px;
      left: 70%;
    }

    .tooltip:hover .tooltiptext {
      visibility: visible;
      text-align: center;
      font-size: 36px;
    }

    .tooltip .tooltiptext a {
      color: #000;
    }
  </style>

	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
	
		<title>TIngyu Mo</title>
			
    <!-- <link rel="icon" type="image/png" href="images/su_icon_1color.png"/> -->
    <link rel="icon" type="image/png" href="images/berkeley_eecs_icon.png"/>

		<!-- CSS -->
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"/>
    <link rel="stylesheet" href="css/style.css" type="text/css" media="screen"/>
    <link rel="stylesheet" href="css/jquery.popup.css" type="text/css"/>
		<!-- ENDS CSS -->

    <!-- Global site tag (gtag.js) -Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47054450-2"></script>
      <script>
    window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

          gtag('config', 'UA-47054450-2');
      </script>
    <!-- End Global site tag (gtag.js) -Google Analytics -->

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-MQ25LJL');</script>
    <!-- End Google Tag Manager -->

	</head>	

	<body>
    <div class="section">
      <table id="personal">
        <tr>
          <td id="basic">
            <div>
              <br/>
              <div class="tooltip" align="center">
                <h1>TIngyu Mo</h1>
                <!-- <span class="tooltiptext"><a href="https://en.wiktionary.org/wiki/%E6%88%BF" target="_blank">房</a> <a href="https://en.wiktionary.org/wiki/%E5%AF%AC" target="_blank">宽</a></span> -->
              </div>
              <br/>
              <h2><b>Email</b>: motingyu [at] buaa [dot] edu</h2>
            </div>
          </td>
        </tr>

        <tr>

          <td id="bio">
            <div>
                I am a Postdoctoral Scholar at the <a href="https://bair.berkeley.edu/">Berkeley Artificial Intelligence Research (BAIR)</a> Lab working with <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>. I received my Ph.D. from <a href="https://www.stanford.edu/">Stanford University</a>, advised by <a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a> and <a href="https://profiles.stanford.edu/silvio-savarese">Silvio Savarese</a>. I received my B.E. degree from <a href="images/tsinghua.jpg">Tsinghua University</a>. I have interned at <a href="https://ai.google/research/teams/brain">Google Brain</a>, <a href="https://blog.x.company/inside-robotics-at-x-caa134ac854b">Google [x] Robotics</a> and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a>. My research is supported by a <a href="https://cccblog.org/2021/07/22/announcing-the-2021-computing-innovation-fellows/">Computing Innovation Fellowship</a>.
<!-- I study robotics, computer vision,and machine learning. My research is supported by a <a href="https://vpge.stanford.edu/fellowships-funding/sgf/details">Stanford Graduate Fellowship</a>. -->
                <!-- Previously, I also worked on physical simulation with <a href="http://physbam.stanford.edu/~fedkiw/">Prof. Ron Fedkiw</a> in the <a href="http://physbam.stanford.edu/">PhysBAM Lab</a>. -->
                <br/><br/>
                <h2>
                  <a href="files/kuan_fang_cv.pdf">CV</a>
                  &nbsp;&nbsp;|&nbsp;&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=BZBkjNYAAAAJ">Google Scholar</a>
                  &nbsp;&nbsp;|&nbsp;&nbsp;
                  <a href="https://github.com/kuanfang/">GitHub</a>
                  &nbsp;&nbsp;|&nbsp;&nbsp;
                  <a href="https://www.linkedin.com/in/kuanfang/">LinkedIn</a>
                  &nbsp;&nbsp;|&nbsp;&nbsp;
                  <a href="https://twitter.com/KuanFang">Twitter</a>
                </h2>
            </div>
          </td>

          <td id="photo">
            <div>
              <img src="images/kuan_fang.jpg"/>
            </div>
          </td>

        </tr>
      </table>
    </div>

    <div id="info">

      <div class="section">
        <h1>News</h1><br/>

        <ul>
          <li>I am honored to have received the <a href="https://cccblog.org/2021/07/22/announcing-the-2021-computing-innovation-fellows/">Computing Innovation Fellowship</a>.</li>
          <li>I started a Postdoc with <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> at the Berkeley AI Research (BAIR) Lab.</li>
          <li>We are organizing <a href="https://rssvlrr.github.io/">Workshop on Visual Learning and Reasoning for Robotics</a> at <a href="https://roboticsconference.org/">RSS 2021</a>.</li>
          <li>We are organizing <a href="https://sites.google.com/view/iros2020dresr">Tutorial on Deep Representation and Estimation of State for Robotics</a> at <a href="https://sites.google.com/view/iros2020dresr">IROS 2020</a>.</li>
          <li>We are organizing <a href="https://rss2020vlrrm.github.io/">Workshop on Visual Learning and Reasoning for Robotic Manipulation</a> at <a href="https://roboticsconference.org/">RSS 2020</a>.</li>
          <li>New <a href="http://ai.stanford.edu/blog/cavin/">SAIL Blog Post</a> about our recent work on Sequential Problem Solving by Hierarchical Planning in Latent Spaces.</li>
          <li>We released <a href="https://github.com/StanfordVL/robovat/">RoboVat</a>: A unified toolkit for simulated and real-world robotic task environments.</li>
        </ul>
      </div>

      <!-- <div class="section"> -->
      <!--   <h1>Preprints</h1><br/> -->
      <!--   <table id="publications"> -->
      <!--  -->
      <!--     <tr> -->
      <!--       <td id="publications-image"> -->
      <!--         <img src="papers/images/atr.gif"/> -->
      <!--       </td> -->
      <!--       <td id="publications-info"> -->
      <!--         <p> -->
      <!--           <span id="paper"> -->
      <!--             Active Task Randomization: Learning Visuomotor Skills for Sequential Manipulation by Proposing Feasible and Novel Tasks -->
      <!--           </span> -->
      <!--           <br/> -->
      <!--           <span id="author"> -->
      <!--             <span id="author-kuan">TIngyu Mo*</span>, -->
      <!--             Toki Migimatsu*,  -->
      <!--             Ajay Mandlekar,  -->
      <!--             Li Fei-Fei,  -->
      <!--             Jeannette Bohg -->
      <!--           </span> -->
      <!--           <br/> -->
      <!--           ArXiv -->
      <!--           <br/> -->
      <!--           <a href="https://arxiv.org/pdf/2211.06134.pdf">PDF</a> | -->
      <!--           <a href="https://sites.google.com/view/active-task-randomization">Website</a> | -->
      <!--           <a href="bibtex/atr2022.bib">BibTex</a> -->
      <!--         </p> -->
      <!--       </td> -->
      <!--     </tr> -->
      <!--  -->
      <!--   </table> -->
      <!-- </div> -->

      <div class="section">
        <h1>Publications</h1><br/>
        <table id="publications">
          <tr>
            <td id="publications-image">
              <img src="papers/images/atr.gif"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Active Task Randomization: Learning Visuomotor Skills for Sequential Manipulation by Proposing Feasible and Novel Tasks
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo*</span>,
                  Toki Migimatsu*, 
                  Ajay Mandlekar, 
                  Li Fei-Fei, 
                  Jeannette Bohg
                </span>
                <br/>
                arXiv Preprint
                <br/>
                <a href="https://arxiv.org/pdf/2211.06134.pdf">PDF</a> |
                <a href="https://sites.google.com/view/active-task-randomization">Website</a> |
                <a href="bibtex/atr2022.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/flap.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo</span>,
                  Patrick Yin, 
                  Ashvin Nair, 
                  Homer Walke, 
                  Gengchen Yan,
                  Sergey Levine
                </span>
                <br/>
                CoRL 2022 <b>(Oral)</b>
                <br/>
                <a href="https://arxiv.org/pdf/2210.06601.pdf">PDF</a> |
                <a href="https://sites.google.com/view/project-flap">Website</a> |
                <a href="https://github.com/kuanfang/flap">Code</a> |
                <a href="bibtex/fang2022flap.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/ptp.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo*</span>,
                  Patrick Yin*, 
                  Ashvin Nair, 
                  Sergey Levine
                  (* indicates equal contribution)
                </span>
                <br/>
                IROS 2022
                <br/>
                <a href="https://arxiv.org/pdf/2205.08129.pdf">PDF</a> |
                <a href="https://sites.google.com/view/planning-to-practice/">Website</a> |
                <a href="https://github.com/patrickhaoy/ptp">Code</a> |
                <a href="bibtex/fang2022ptp.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/slide.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Discovering Generalizable Skills via Automated Generation of Diverse Tasks
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo</span>,
                  Yuke Zhu,
                  Silvio Savarese,
                  Li Fei-Fei
                </span>
                <br/>
                RSS 2021
                <br/>
                <a href="https://arxiv.org/pdf/2106.13935.pdf">PDF</a> |
                <a href="https://sites.google.com/view/rss-slide/">Website</a> |
                <a href="bibtex/fang2021slide.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/giga.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations
                </span>
                <br/>
                <span id="author">
                  Zhenyu Jiang, 
                  Yifeng Zhu, 
                  Maxwell Svetlik, 
                  <span id="author-kuan">TIngyu Mo</span>,
                  Yuke Zhu,
                </span>
                <br/>
                RSS 2021
                <br/>
                <a href="https://arxiv.org/pdf/2104.01542.pdf">PDF</a> |
                <a href="https://sites.google.com/view/rpl-giga2021/">Website</a> |
                <a href="https://github.com/UT-Austin-RPL/GIGA">Code</a> |
                <a href="bibtex/jiang2021giga.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/apt-gen.gif"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Adaptive Procedural Task Generation for Hard-Exploration Problems
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo</span>,
                  Yuke Zhu,
                  Silvio Savarese,
                  Li Fei-Fei
                </span>
                <br/>
                ICLR 2021
                <br/>
                <a href="https://arxiv.org/pdf/2007.00350.pdf">PDF</a> |
                <a href="https://apt-gen.github.io/">Website</a> |
                <a href="bibtex/fang2020aptgen.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/keto.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  KETO: Learning Keypoint Representations for Tool Manipulation
                </span>
                <br/>
                <span id="author">
                  Zengyi Qin,
                  <span id="author-kuan">TIngyu Mo</span>,
                  Yuke Zhu,
                  Li Fei-Fei,
                  Silvio Savarese
                </span>
                <br/>
                ICRA 2020
                <br/>
                <a href="https://arxiv.org/pdf/1910.11977.pdf">PDF</a> |
                <a href="https://sites.google.com/view/ke-to">Website</a> |
                <a href="https://github.com/stanfordvl/keto">Code</a> |
                <a href="bibtex/qin2019keto.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/cavin.gif"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Dynamics Learning with Cascaded Variational Inference for Multi-Step Manipulation
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo</span>,
                  Yuke Zhu,
                  Animesh Garg,
                  Silvio Savarese,
                  Li Fei-Fei
                </span>
                <br/>
                CoRL 2019 <b>(Oral)</b>
                <br/>
                <a href="https://arxiv.org/pdf/1910.13395.pdf">PDF</a> |
                <a href="http://pair.stanford.edu/cavin/">Website</a> |
                <a href="http://ai.stanford.edu/blog/cavin/">Blog</a> |
                <a href="https://github.com/StanfordVL/robovat/">Environment</a> |
                <a href="https://github.com/StanfordVL/cavin/">Code</a> |
                <a href="bibtex/fang2019cavin.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/smt.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo,</span>
                  Alexander Toshev,
                  Li Fei-Fei, 
                  Silvio Savarese
                </span>
                <br/>
                CVPR 2019
                <br/>
                <a href="https://arxiv.org/pdf/1903.03878.pdf">PDF</a> |
                <a href="https://sites.google.com/view/scene-memory-transformer">Website</a> |
                <a href="bibtex/fang2019smt.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/tognet.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo,</span>
                  Yuke Zhu,
                  Animesh Garg,
                  Andrey Kurenkov,
                  Viraj Mehta,
                  Li Fei-Fei,
                  Silvio Savarese
                </span>
                <br/>
                RSS 2018 (Journal version in IJRR 2019)
                <br/>
                <a href="https://arxiv.org/pdf/1806.09266.pdf">PDF (RSS 2018)</a> |
                <a href="https://doi.org/10.1177/0278364919872545">PDF (IJRR 2019)</a> |
                <a href="https://sites.google.com/view/task-oriented-grasp/">Website</a> |
                <a href="https://www.youtube.com/watch?v=vYExCFeKXhw&feature=youtu.be">Video</a> |
                <a href="bibtex/fang2018tog.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/mtda.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo</span>,
                  Yunfei Bai, 
                  Stefan Hinterstoisser, 
                  Silvio Savarese, 
                  Mrinal Kalakrishnan
                </span>
                <br/>
                ICRA 2018
                <br/>
                <a href="https://arxiv.org/pdf/1710.06422.pdf">PDF</a> |
                <a href="https://sites.google.com/view/multi-task-domain-adaptation/">Website</a> |
                <a href="https://www.youtube.com/watch?v=BR5bgPxjvRM">Video</a> |
                <a href="bibtex/fang2018mtda.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/demo2vec.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Demo2Vec: Reasoning Object Affordances from Online Videos
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo</span>*,
                  Te-Lin Wu*, 
                  Daniel Yang, 
                  Silvio Savarese, 
                  Joseph J. Lim 
                  (* indicates equal contribution)
                </span>
                <br/>
                CVPR 2018
                <br/>
                <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Demo2Vec_Reasoning_Object_CVPR_2018_paper.pdf">PDF</a> |
                <a href="https://sites.google.com/view/demo2vec/">Website</a> |
                <a href="https://github.com/kuanfang/opra">Code</a> |
                <a href="bibtex/fang2018demo2vec.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/ran.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Recurrent Autoregressive Networks for Online Multi-Object Tracking
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">TIngyu Mo</span>,
                  Yu Xiang, 
                  Xiaocheng Li, 
                  Silvio Savarese
                </span>
                <br/>
                WACV 2018
                <br/>
                <a href="https://arxiv.org/pdf/1711.02741.pdf">PDF</a> |
                <a href="https://www.youtube.com/watch?v=2Wbo9eky6jY">Video</a> |
                <a href="bibtex/fang2018ran.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/delay.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  DeLay: Robust Spatial Layout Estimation for Cluttered Indoor Scenes
                </span>
                <br/>
                <span id="author">
                  Saumitro Dasgupta,
                  <span id="author-kuan">TIngyu Mo</span>*,
                   Kevin Chen*, 
                   Silvio Savarese 
                   (* indicates equal contribution)
                </span>
                <br/>
                CVPR 2016
                <br/>
                <a href="http://svl.stanford.edu/assets/papers/delay-robust-spatial.pdf">PDF</a> |
                <a href="http://deeplayout.stanford.edu/">Website</a> |
                <a href="bibtex/dasgupta2016delay.bib">BibTex</a>
              </p>
            </td>
          </tr>

        </table>
      </div>

      <div class="section">
        <h1>Awards and Honors</h1><br/>

        <ul>
          <li><a href="https://cccblog.org/2021/07/22/announcing-the-2021-computing-innovation-fellows/">Computing Innovation Fellowship</a>, 2021 - 2022</li>
          <li><a href="https://vpge.stanford.edu/fellowships-funding/sgf/named-fellowships">Stanford Graduate Fellowship</a>, 2014 - 2017</li>
          <!-- <li><a href="https://vpge.stanford.edu/fellowships-funding/sgf/named-fellowships">Stanford Graduate Fellowship (David Cheriton Fellow)</a>, 2014 - 2017</li> -->
          <li>Award of Excellence in the <a href="https://www.microsoft.com/en-us/research/group/machine-learning-research-group/">Microsoft Research Asia (MSRA)</a> Internship Program, 2014</li>
          <!-- <li>Best Paper Award at the 31st IEEE International Conference on Computer Design (ICCD), 2013</li> -->
          <li>Member of <a href="http://www.tuef.tsinghua.edu.cn/column/sp1">Spark Innovative Talent Cultivation Program</a>, Tsinghua University, 2012 - 2014</li>
          <li>Comprehensive Scholarship for Academic Excellence, Tsinghua University, 2012 and 2013</li>
        </ul>
      </div>

      <div class="section">
        <h1>Teaching</h1><br/>

        <ul>
          <li>Teaching Assistant, <a href="http://web.stanford.edu/class/cs231a/">Computer Vision, From 3D Reconstruction to Recognition (CS231A)</a>, Stanford University, 2021</li>
          <li>Teaching Assistant, <a href="http://web.stanford.edu/class/cs231a/">Computer Vision, From 3D Reconstruction to Recognition (CS231A)</a>, Stanford University, 2018</li>
          <li>Instructor, <a href="https://ai4all.spcs.stanford.edu/">Stanford AI4ALL Program</a>, Stanford University, 2020</li>
        </ul>
      </div>

    </div>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <p align="left">
            <font size="2">
              <a href="https://people.eecs.berkeley.edu/~barron/">Website template courtesy</a>
            </font>
          </p>
        </td>
      </tr>
    </table>

    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MQ25LJL"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

	</body>
</html>
